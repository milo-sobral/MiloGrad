{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Nets and Backprop\n",
    "\n",
    "Following along this video: https://www.youtube.com/watch?v=VMj-3S1tku0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root, format='svg', rankdir='LR'):\n",
    "    \"\"\"\n",
    "    format: png | svg | ...\n",
    "    rankdir: TB (top to bottom graph) | LR (left to right)\n",
    "    \"\"\"\n",
    "    assert rankdir in ['LR', 'TB']\n",
    "    nodes, edges = trace(root)\n",
    "    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n",
    "    \n",
    "    for n in nodes:\n",
    "        dot.node(name=str(id(n)), label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
    "        if n._op:\n",
    "            dot.node(name=str(id(n)) + n._op, label=n._op)\n",
    "            dot.edge(str(id(n)) + n._op, str(id(n)))\n",
    "    \n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "    \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to integrate without having to express everything in our neural network as a big expression to differenciate analytically so we'll use a representation of Value to do so iteratively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        # This is a set for efficiency\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        self._backward = lambda: None\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={str(self.data)})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"Power only supports int/float for now...\"\n",
    "        out = Value(self.data**other, (self, ), f\"**{other}\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = other * self.data**(other-1) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * other**-1\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self, ), 'exp')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.grad = 1.0\n",
    "\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "    \n",
    "    def tanh(self):\n",
    "        n = self.data\n",
    "        t = (math.exp(2 * n) - 1) / (math.exp(2 * n) + 1)\n",
    "        out = Value(t, _children=(self, ), _op='tanh')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(-3.0, label='a')\n",
    "b = Value(2.0, label='b')\n",
    "c = Value(10.0, label='c')\n",
    "e = a * b; e.label = 'e'\n",
    "d = e + c; d.label = 'd'\n",
    "f = Value(-2.0, label='f')\n",
    "L = d * f; L.label = 'L'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"1342pt\" height=\"154pt\"\n",
       " viewBox=\"0.00 0.00 1342.00 154.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 150)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-150 1338,-150 1338,4 -4,4\"/>\n",
       "<!-- 140309218086496 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140309218086496</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"735,-54.5 735,-90.5 971,-90.5 971,-54.5 735,-54.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"748\" y=\"-68.8\" font-family=\"Times,serif\" font-size=\"14.00\">d</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"761,-54.5 761,-90.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"812\" y=\"-68.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 4.0000</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"863,-54.5 863,-90.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"917\" y=\"-68.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad &#45;2.0000</text>\n",
       "</g>\n",
       "<!-- 140309218170192* -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>140309218170192*</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1034\" cy=\"-99.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1034\" y=\"-95.8\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n",
       "</g>\n",
       "<!-- 140309218086496&#45;&gt;140309218170192* -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>140309218086496&#45;&gt;140309218170192*</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M971.15,-90.17C980.53,-91.58 989.46,-92.93 997.46,-94.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"997.01,-97.61 1007.42,-95.64 998.06,-90.69 997.01,-97.61\"/>\n",
       "</g>\n",
       "<!-- 140309218086496+ -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140309218086496+</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"672\" cy=\"-72.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"672\" y=\"-68.8\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- 140309218086496+&#45;&gt;140309218086496 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140309218086496+&#45;&gt;140309218086496</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M699.03,-72.5C706.58,-72.5 715.38,-72.5 724.87,-72.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"724.99,-76 734.99,-72.5 724.99,-69 724.99,-76\"/>\n",
       "</g>\n",
       "<!-- 140309218173024 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140309218173024</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"736.5,-109.5 736.5,-145.5 969.5,-145.5 969.5,-109.5 736.5,-109.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"747.5\" y=\"-123.8\" font-family=\"Times,serif\" font-size=\"14.00\">f</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"758.5,-109.5 758.5,-145.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"812\" y=\"-123.8\" font-family=\"Times,serif\" font-size=\"14.00\">data &#45;2.0000</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"865.5,-109.5 865.5,-145.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"917.5\" y=\"-123.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 4.0000</text>\n",
       "</g>\n",
       "<!-- 140309218173024&#45;&gt;140309218170192* -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>140309218173024&#45;&gt;140309218170192*</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M969.13,-109.49C979.27,-107.91 988.92,-106.4 997.51,-105.05\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"998.09,-108.5 1007.43,-103.5 997.01,-101.59 998.09,-108.5\"/>\n",
       "</g>\n",
       "<!-- 140309218046128 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140309218046128</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"4,-55.5 4,-91.5 236,-91.5 236,-55.5 4,-55.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"17\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">b</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"30,-55.5 30,-91.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"81\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 2.0000</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"132,-55.5 132,-91.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"184\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 6.0000</text>\n",
       "</g>\n",
       "<!-- 140309218042624* -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>140309218042624*</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"303\" cy=\"-45.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"303\" y=\"-41.8\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n",
       "</g>\n",
       "<!-- 140309218046128&#45;&gt;140309218042624* -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>140309218046128&#45;&gt;140309218042624*</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M236.14,-55.69C246.97,-54.01 257.28,-52.42 266.4,-51.01\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"267.17,-54.43 276.52,-49.44 266.1,-47.51 267.17,-54.43\"/>\n",
       "</g>\n",
       "<!-- 140309218038976 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>140309218038976</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"0,-0.5 0,-36.5 240,-36.5 240,-0.5 0,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"12.5\" y=\"-14.8\" font-family=\"Times,serif\" font-size=\"14.00\">a</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"25,-0.5 25,-36.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"78.5\" y=\"-14.8\" font-family=\"Times,serif\" font-size=\"14.00\">data &#45;3.0000</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"132,-0.5 132,-36.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"186\" y=\"-14.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad &#45;4.0000</text>\n",
       "</g>\n",
       "<!-- 140309218038976&#45;&gt;140309218042624* -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>140309218038976&#45;&gt;140309218042624*</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M240.22,-36.28C249.6,-37.68 258.53,-39.02 266.54,-40.21\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"266.08,-43.68 276.48,-41.69 267.11,-36.76 266.08,-43.68\"/>\n",
       "</g>\n",
       "<!-- 140309218046176 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>140309218046176</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"366,-82.5 366,-118.5 609,-118.5 609,-82.5 366,-82.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"378\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\">c</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"390,-82.5 390,-118.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"445.5\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 10.0000</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"501,-82.5 501,-118.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"555\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad &#45;2.0000</text>\n",
       "</g>\n",
       "<!-- 140309218046176&#45;&gt;140309218086496+ -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>140309218046176&#45;&gt;140309218086496+</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M605.89,-82.49C616.53,-80.86 626.65,-79.3 635.61,-77.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"636.2,-81.38 645.55,-76.4 635.14,-74.46 636.2,-81.38\"/>\n",
       "</g>\n",
       "<!-- 140309218042624 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>140309218042624</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"367.5,-27.5 367.5,-63.5 607.5,-63.5 607.5,-27.5 367.5,-27.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"380\" y=\"-41.8\" font-family=\"Times,serif\" font-size=\"14.00\">e</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"392.5,-27.5 392.5,-63.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"446\" y=\"-41.8\" font-family=\"Times,serif\" font-size=\"14.00\">data &#45;6.0000</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"499.5,-27.5 499.5,-63.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"553.5\" y=\"-41.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad &#45;2.0000</text>\n",
       "</g>\n",
       "<!-- 140309218042624&#45;&gt;140309218086496+ -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>140309218042624&#45;&gt;140309218086496+</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M607.68,-63.13C617.57,-64.6 626.98,-65.99 635.37,-67.23\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"635.02,-70.71 645.42,-68.72 636.04,-63.79 635.02,-70.71\"/>\n",
       "</g>\n",
       "<!-- 140309218042624*&#45;&gt;140309218042624 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140309218042624*&#45;&gt;140309218042624</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M330.15,-45.5C337.98,-45.5 347.16,-45.5 357.06,-45.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"357.19,-49 367.19,-45.5 357.19,-42 357.19,-49\"/>\n",
       "</g>\n",
       "<!-- 140309218170192 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>140309218170192</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1097,-81.5 1097,-117.5 1334,-117.5 1334,-81.5 1097,-81.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1110\" y=\"-95.8\" font-family=\"Times,serif\" font-size=\"14.00\">L</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"1123,-81.5 1123,-117.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"1176.5\" y=\"-95.8\" font-family=\"Times,serif\" font-size=\"14.00\">data &#45;8.0000</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"1230,-81.5 1230,-117.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"1282\" y=\"-95.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 1.0000</text>\n",
       "</g>\n",
       "<!-- 140309218170192*&#45;&gt;140309218170192 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140309218170192*&#45;&gt;140309218170192</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1061.1,-99.5C1068.61,-99.5 1077.33,-99.5 1086.75,-99.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1086.78,-103 1096.78,-99.5 1086.78,-96 1086.78,-103\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f9c49360560>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1, 1))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        act = sum((wi * xi for wi, xi in zip(self.w,x)), self.b)\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "    \n",
    "class Layer:\n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "    \n",
    "class MLP:\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-0.3408454351996858)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2.0, 3.0]\n",
    "model = MLP(3, [4, 4, 1])\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a toy dataset\n",
    "def toy_dataset(n=1000):\n",
    "    X = [[random.uniform(-1, 1), random.uniform(-1, 1), random.uniform(-1, 1)] for _ in range(n)]\n",
    "    # either 1 or 0 randomly\n",
    "    y = [random.choice([-1.0, 1.0]) for _ in range(n)]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = toy_dataset(10)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining our loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=26.561972602058415)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = sum((yout - ygt)**2 for ygt, yout in zip(y, y_preds))\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0 : 21.796531790607006\n",
      "Loss at epoch 1 : 17.898923046444175\n",
      "Loss at epoch 2 : 12.613978926969205\n",
      "Loss at epoch 3 : 9.770398981042561\n",
      "Loss at epoch 4 : 9.20731117815905\n",
      "Loss at epoch 5 : 9.028032642066146\n",
      "Loss at epoch 6 : 8.92194966471972\n",
      "Loss at epoch 7 : 8.838706696037923\n",
      "Loss at epoch 8 : 8.766026550778513\n",
      "Loss at epoch 9 : 8.699934276235673\n",
      "Loss at epoch 10 : 8.638701369159836\n",
      "Loss at epoch 11 : 8.58129922242116\n",
      "Loss at epoch 12 : 8.526957557127758\n",
      "Loss at epoch 13 : 8.475030517300706\n",
      "Loss at epoch 14 : 8.42495249928762\n",
      "Loss at epoch 15 : 8.376220114627264\n",
      "Loss at epoch 16 : 8.328381758672371\n",
      "Loss at epoch 17 : 8.28102972959603\n",
      "Loss at epoch 18 : 8.233793740494004\n",
      "Loss at epoch 19 : 8.186335696982939\n",
      "Loss at epoch 20 : 8.138345837186936\n",
      "Loss at epoch 21 : 8.08954036004804\n",
      "Loss at epoch 22 : 8.03966066238714\n",
      "Loss at epoch 23 : 7.988474293381556\n",
      "Loss at epoch 24 : 7.935777708389187\n",
      "Loss at epoch 25 : 7.881400843537527\n",
      "Loss at epoch 26 : 7.825213417131694\n",
      "Loss at epoch 27 : 7.767132675097271\n",
      "Loss at epoch 28 : 7.707132026357424\n",
      "Loss at epoch 29 : 7.645249671974835\n",
      "Loss at epoch 30 : 7.581595965054867\n",
      "Loss at epoch 31 : 7.51635793731147\n",
      "Loss at epoch 32 : 7.449799324483623\n",
      "Loss at epoch 33 : 7.38225466015482\n",
      "Loss at epoch 34 : 7.314116681866929\n",
      "Loss at epoch 35 : 7.245817376602172\n",
      "Loss at epoch 36 : 7.177804280107283\n",
      "Loss at epoch 37 : 7.110514777670417\n",
      "Loss at epoch 38 : 7.044351742089514\n",
      "Loss at epoch 39 : 6.979663643526563\n",
      "Loss at epoch 40 : 6.916731313199085\n",
      "Loss at epoch 41 : 6.855762155050492\n",
      "Loss at epoch 42 : 6.796891224504051\n",
      "Loss at epoch 43 : 6.7401876080990375\n",
      "Loss at epoch 44 : 6.685664111718068\n",
      "Loss at epoch 45 : 6.633288348536954\n",
      "Loss at epoch 46 : 6.582993727008674\n",
      "Loss at epoch 47 : 6.534689365845727\n",
      "Loss at epoch 48 : 6.488268450082244\n",
      "Loss at epoch 49 : 6.443614909065709\n",
      "Loss at epoch 50 : 6.400608526985801\n",
      "Loss at epoch 51 : 6.359128710506258\n",
      "Loss at epoch 52 : 6.3190571712380414\n",
      "Loss at epoch 53 : 6.280279766719898\n",
      "Loss at epoch 54 : 6.242687707633413\n",
      "Loss at epoch 55 : 6.206178297358335\n",
      "Loss at epoch 56 : 6.170655331347632\n",
      "Loss at epoch 57 : 6.1360292517471295\n",
      "Loss at epoch 58 : 6.102217127812183\n",
      "Loss at epoch 59 : 6.069142514158201\n",
      "Loss at epoch 60 : 6.036735225403568\n",
      "Loss at epoch 61 : 6.004931056011113\n",
      "Loss at epoch 62 : 5.973671467016976\n",
      "Loss at epoch 63 : 5.942903256036419\n",
      "Loss at epoch 64 : 5.9125782228848855\n",
      "Loss at epoch 65 : 5.882652839973272\n",
      "Loss at epoch 66 : 5.853087934089875\n",
      "Loss at epoch 67 : 5.823848384118449\n",
      "Loss at epoch 68 : 5.7949028375684755\n",
      "Loss at epoch 69 : 5.766223447446937\n",
      "Loss at epoch 70 : 5.737785629934124\n",
      "Loss at epoch 71 : 5.709567842501251\n",
      "Loss at epoch 72 : 5.681551381490479\n",
      "Loss at epoch 73 : 5.653720197737025\n",
      "Loss at epoch 74 : 5.626060728518781\n",
      "Loss at epoch 75 : 5.598561743944884\n",
      "Loss at epoch 76 : 5.571214205816922\n",
      "Loss at epoch 77 : 5.544011136994731\n",
      "Loss at epoch 78 : 5.516947499355633\n",
      "Loss at epoch 79 : 5.490020078537684\n",
      "Loss at epoch 80 : 5.4632273737925825\n",
      "Loss at epoch 81 : 5.436569491433771\n",
      "Loss at epoch 82 : 5.4100480405429625\n",
      "Loss at epoch 83 : 5.383666029788433\n",
      "Loss at epoch 84 : 5.357427764406671\n",
      "Loss at epoch 85 : 5.331338742601574\n",
      "Loss at epoch 86 : 5.305405550819129\n",
      "Loss at epoch 87 : 5.279635757557035\n",
      "Loss at epoch 88 : 5.25403780556521\n",
      "Loss at epoch 89 : 5.228620902481044\n",
      "Loss at epoch 90 : 5.203394910119583\n",
      "Loss at epoch 91 : 5.178370232800389\n",
      "Loss at epoch 92 : 5.15355770523632\n",
      "Loss at epoch 93 : 5.128968480632304\n",
      "Loss at epoch 94 : 5.1046139197416185\n",
      "Loss at epoch 95 : 5.080505481701471\n",
      "Loss at epoch 96 : 5.056654617517228\n",
      "Loss at epoch 97 : 5.033072667085068\n",
      "Loss at epoch 98 : 5.009770760636135\n",
      "Loss at epoch 99 : 4.986759725452464\n",
      "Loss at epoch 100 : 4.964049998647948\n",
      "Loss at epoch 101 : 4.941651546728737\n",
      "Loss at epoch 102 : 4.91957379255004\n",
      "Loss at epoch 103 : 4.897825550173901\n",
      "Loss at epoch 104 : 4.876414968009259\n",
      "Loss at epoch 105 : 4.855349480485499\n",
      "Loss at epoch 106 : 4.83463576837817\n",
      "Loss at epoch 107 : 4.814279727774493\n",
      "Loss at epoch 108 : 4.794286447540351\n",
      "Loss at epoch 109 : 4.774660195033206\n",
      "Loss at epoch 110 : 4.755404409699208\n",
      "Loss at epoch 111 : 4.736521704100049\n",
      "Loss at epoch 112 : 4.718013871837361\n",
      "Loss at epoch 113 : 4.69988190178038\n",
      "Loss at epoch 114 : 4.682125997956668\n",
      "Loss at epoch 115 : 4.664745604435627\n",
      "Loss at epoch 116 : 4.64773943451945\n",
      "Loss at epoch 117 : 4.631105503555162\n",
      "Loss at epoch 118 : 4.614841164692933\n",
      "Loss at epoch 119 : 4.598943146938314\n",
      "Loss at epoch 120 : 4.583407594877653\n",
      "Loss at epoch 121 : 4.568230109495072\n",
      "Loss at epoch 122 : 4.553405789544011\n",
      "Loss at epoch 123 : 4.538929272984958\n",
      "Loss at epoch 124 : 4.524794778052006\n",
      "Loss at epoch 125 : 4.510996143562784\n",
      "Loss at epoch 126 : 4.497526868138099\n",
      "Loss at epoch 127 : 4.484380148047951\n",
      "Loss at epoch 128 : 4.47154891344892\n",
      "Loss at epoch 129 : 4.4590258628234185\n",
      "Loss at epoch 130 : 4.446803495473409\n",
      "Loss at epoch 131 : 4.434874141959874\n",
      "Loss at epoch 132 : 4.423229992413963\n",
      "Loss at epoch 133 : 4.411863122676701\n",
      "Loss at epoch 134 : 4.400765518251091\n",
      "Loss at epoch 135 : 4.3899290960737485\n",
      "Loss at epoch 136 : 4.379345724132862\n",
      "Loss at epoch 137 : 4.369007238975671\n",
      "Loss at epoch 138 : 4.358905461161873\n",
      "Loss at epoch 139 : 4.349032208729829\n",
      "Loss at epoch 140 : 4.339379308750447\n",
      "Loss at epoch 141 : 4.329938607049205\n",
      "Loss at epoch 142 : 4.3207019761807395\n",
      "Loss at epoch 143 : 4.3116613217424185\n",
      "Loss at epoch 144 : 4.302808587114235\n",
      "Loss at epoch 145 : 4.294135756711918\n",
      "Loss at epoch 146 : 4.285634857839042\n",
      "Loss at epoch 147 : 4.277297961222029\n",
      "Loss at epoch 148 : 4.269117180309747\n",
      "Loss at epoch 149 : 4.261084669416916\n",
      "Loss at epoch 150 : 4.253192620788075\n",
      "Loss at epoch 151 : 4.245433260656586\n",
      "Loss at epoch 152 : 4.237798844371238\n",
      "Loss at epoch 153 : 4.2302816506615954\n",
      "Loss at epoch 154 : 4.22287397511264\n",
      "Loss at epoch 155 : 4.215568122919577\n",
      "Loss at epoch 156 : 4.208356400995002\n",
      "Loss at epoch 157 : 4.201231109503553\n",
      "Loss at epoch 158 : 4.194184532903435\n",
      "Loss at epoch 159 : 4.1872089305804705\n",
      "Loss at epoch 160 : 4.18029652716866\n",
      "Loss at epoch 161 : 4.173439502661975\n",
      "Loss at epoch 162 : 4.166629982435624\n",
      "Loss at epoch 163 : 4.15986002731155\n",
      "Loss at epoch 164 : 4.153121623823097\n",
      "Loss at epoch 165 : 4.1464066748574675\n",
      "Loss at epoch 166 : 4.139706990882841\n",
      "Loss at epoch 167 : 4.13301428199959\n",
      "Loss at epoch 168 : 4.126320151092573\n",
      "Loss at epoch 169 : 4.119616088404166\n",
      "Loss at epoch 170 : 4.1128934678955185\n",
      "Loss at epoch 171 : 4.106143545816566\n",
      "Loss at epoch 172 : 4.099357461963057\n",
      "Loss at epoch 173 : 4.092526244160655\n",
      "Loss at epoch 174 : 4.085640816580714\n",
      "Loss at epoch 175 : 4.078692012557691\n",
      "Loss at epoch 176 : 4.071670592641566\n",
      "Loss at epoch 177 : 4.064567268676312\n",
      "Loss at epoch 178 : 4.057372734742098\n",
      "Loss at epoch 179 : 4.050077705828016\n",
      "Loss at epoch 180 : 4.042672965105168\n",
      "Loss at epoch 181 : 4.035149420636624\n",
      "Loss at epoch 182 : 4.027498172278747\n",
      "Loss at epoch 183 : 4.019710589383347\n",
      "Loss at epoch 184 : 4.011778399686571\n",
      "Loss at epoch 185 : 4.003693789451505\n",
      "Loss at epoch 186 : 3.9954495145012956\n",
      "Loss at epoch 187 : 3.9870390212243287\n",
      "Loss at epoch 188 : 3.978456575943593\n",
      "Loss at epoch 189 : 3.9696974002183105\n",
      "Loss at epoch 190 : 3.9607578086986557\n",
      "Loss at epoch 191 : 3.9516353451125634\n",
      "Loss at epoch 192 : 3.9423289108772974\n",
      "Loss at epoch 193 : 3.9328388797728238\n",
      "Loss at epoch 194 : 3.9231671911918937\n",
      "Loss at epoch 195 : 3.9133174138214555\n",
      "Loss at epoch 196 : 3.903294771360016\n",
      "Loss at epoch 197 : 3.893106122191262\n",
      "Loss at epoch 198 : 3.8827598859591297\n",
      "Loss at epoch 199 : 3.8722659118290865\n",
      "Loss at epoch 200 : 3.861635285911819\n",
      "Loss at epoch 201 : 3.850880078808297\n",
      "Loss at epoch 202 : 3.840013038327758\n",
      "Loss at epoch 203 : 3.829047236821652\n",
      "Loss at epoch 204 : 3.8179956868411766\n",
      "Loss at epoch 205 : 3.8068709424624476\n",
      "Loss at epoch 206 : 3.7956847061220644\n",
      "Loss at epoch 207 : 3.7844474617314114\n",
      "Loss at epoch 208 : 3.773168153917267\n",
      "Loss at epoch 209 : 3.761853930425689\n",
      "Loss at epoch 210 : 3.7505099602468412\n",
      "Loss at epoch 211 : 3.739139334339355\n",
      "Loss at epoch 212 : 3.727743049605107\n",
      "Loss at epoch 213 : 3.7163200707165323\n",
      "Loss at epoch 214 : 3.7048674592116164\n",
      "Loss at epoch 215 : 3.6933805554738535\n",
      "Loss at epoch 216 : 3.681853197102337\n",
      "Loss at epoch 217 : 3.670277956790625\n",
      "Loss at epoch 218 : 3.658646383979764\n",
      "Loss at epoch 219 : 3.6469492368692564\n",
      "Loss at epoch 220 : 3.6351766944120003\n",
      "Loss at epoch 221 : 3.623318541233829\n",
      "Loss at epoch 222 : 3.611364321615853\n",
      "Loss at epoch 223 : 3.599303461471568\n",
      "Loss at epoch 224 : 3.5871253594707695\n",
      "Loss at epoch 225 : 3.574819450046798\n",
      "Loss at epoch 226 : 3.562375241997744\n",
      "Loss at epoch 227 : 3.5497823368385992\n",
      "Loss at epoch 228 : 3.537030431093208\n",
      "Loss at epoch 229 : 3.5241093064528175\n",
      "Loss at epoch 230 : 3.5110088112836624\n",
      "Loss at epoch 231 : 3.4977188364324245\n",
      "Loss at epoch 232 : 3.4842292877267633\n",
      "Loss at epoch 233 : 3.4705300570472906\n",
      "Loss at epoch 234 : 3.4566109933872156\n",
      "Loss at epoch 235 : 3.442461874931118\n",
      "Loss at epoch 236 : 3.428072382878142\n",
      "Loss at epoch 237 : 3.4134320775037432\n",
      "Loss at epoch 238 : 3.3985303767894317\n",
      "Loss at epoch 239 : 3.3833565378418533\n",
      "Loss at epoch 240 : 3.367899641260163\n",
      "Loss at epoch 241 : 3.3521485785842478\n",
      "Loss at epoch 242 : 3.3360920429568695\n",
      "Loss at epoch 243 : 3.3197185231528845\n",
      "Loss at epoch 244 : 3.3030163011621467\n",
      "Loss at epoch 245 : 3.2859734535545453\n",
      "Loss at epoch 246 : 3.268577856902097\n",
      "Loss at epoch 247 : 3.250817197580706\n",
      "Loss at epoch 248 : 3.2326789863208774\n",
      "Loss at epoch 249 : 3.214150577919903\n",
      "Loss at epoch 250 : 3.195219196565918\n",
      "Loss at epoch 251 : 3.1758719672549223\n",
      "Loss at epoch 252 : 3.1560959538031974\n",
      "Loss at epoch 253 : 3.135878203967665\n",
      "Loss at epoch 254 : 3.115205802183137\n",
      "Loss at epoch 255 : 3.0940659304058586\n",
      "Loss at epoch 256 : 3.072445937514479\n",
      "Loss at epoch 257 : 3.0503334176599712\n",
      "Loss at epoch 258 : 3.0277162978723515\n",
      "Loss at epoch 259 : 3.0045829351217694\n",
      "Loss at epoch 260 : 2.9809222228921968\n",
      "Loss at epoch 261 : 2.956723707156173\n",
      "Loss at epoch 262 : 2.931977711437397\n",
      "Loss at epoch 263 : 2.906675470414913\n",
      "Loss at epoch 264 : 2.8808092712593543\n",
      "Loss at epoch 265 : 2.854372601601378\n",
      "Loss at epoch 266 : 2.827360302719884\n",
      "Loss at epoch 267 : 2.7997687262100404\n",
      "Loss at epoch 268 : 2.7715958920576322\n",
      "Loss at epoch 269 : 2.7428416457183307\n",
      "Loss at epoch 270 : 2.7135078114915814\n",
      "Loss at epoch 271 : 2.6835983392041367\n",
      "Loss at epoch 272 : 2.6531194409939234\n",
      "Loss at epoch 273 : 2.622079714827848\n",
      "Loss at epoch 274 : 2.5904902513131383\n",
      "Loss at epoch 275 : 2.5583647203857978\n",
      "Loss at epoch 276 : 2.5257194345931735\n",
      "Loss at epoch 277 : 2.4925733859388965\n",
      "Loss at epoch 278 : 2.4589482536306013\n",
      "Loss at epoch 279 : 2.4248683805614144\n",
      "Loss at epoch 280 : 2.390360716956965\n",
      "Loss at epoch 281 : 2.3554547303157167\n",
      "Loss at epoch 282 : 2.320182281541019\n",
      "Loss at epoch 283 : 2.2845774679829343\n",
      "Loss at epoch 284 : 2.24867643494608\n",
      "Loss at epoch 285 : 2.212517158044485\n",
      "Loss at epoch 286 : 2.176139199561855\n",
      "Loss at epoch 287 : 2.1395834426735947\n",
      "Loss at epoch 288 : 2.102891807975618\n",
      "Loss at epoch 289 : 2.0661069572196156\n",
      "Loss at epoch 290 : 2.0292719894559874\n",
      "Loss at epoch 291 : 1.9924301349225328\n",
      "Loss at epoch 292 : 1.9556244519854185\n",
      "Loss at epoch 293 : 1.9188975322430861\n",
      "Loss at epoch 294 : 1.8822912185555976\n",
      "Loss at epoch 295 : 1.8458463402802963\n",
      "Loss at epoch 296 : 1.8096024694036785\n",
      "Loss at epoch 297 : 1.7735977005874861\n",
      "Loss at epoch 298 : 1.737868457424758\n",
      "Loss at epoch 299 : 1.7024493264594072\n",
      "Loss at epoch 300 : 1.6673729197906484\n",
      "Loss at epoch 301 : 1.632669766387484\n",
      "Loss at epoch 302 : 1.5983682316009875\n",
      "Loss at epoch 303 : 1.5644944638009206\n",
      "Loss at epoch 304 : 1.5310723665902346\n",
      "Loss at epoch 305 : 1.4981235946733076\n",
      "Loss at epoch 306 : 1.4656675711731042\n",
      "Loss at epoch 307 : 1.4337215240062062\n",
      "Loss at epoch 308 : 1.4023005388265182\n",
      "Loss at epoch 309 : 1.3714176260295194\n",
      "Loss at epoch 310 : 1.3410837993580023\n",
      "Loss at epoch 311 : 1.311308163755481\n",
      "Loss at epoch 312 : 1.2820980102624846\n",
      "Loss at epoch 313 : 1.2534589159318583\n",
      "Loss at epoch 314 : 1.2253948469408398\n",
      "Loss at epoch 315 : 1.197908263290314\n",
      "Loss at epoch 316 : 1.171000223696585\n",
      "Loss at epoch 317 : 1.1446704894915687\n",
      "Loss at epoch 318 : 1.1189176265478724\n",
      "Loss at epoch 319 : 1.093739104431958\n",
      "Loss at epoch 320 : 1.0691313921587022\n",
      "Loss at epoch 321 : 1.0450900500727744\n",
      "Loss at epoch 322 : 1.0216098175156836\n",
      "Loss at epoch 323 : 0.9986846960522779\n",
      "Loss at epoch 324 : 0.9763080281278256\n",
      "Loss at epoch 325 : 0.9544725711075867\n",
      "Loss at epoch 326 : 0.9331705667165938\n",
      "Loss at epoch 327 : 0.9123938059497508\n",
      "Loss at epoch 328 : 0.8921336895630445\n",
      "Loss at epoch 329 : 0.8723812842872342\n",
      "Loss at epoch 330 : 0.8531273749274859\n",
      "Loss at epoch 331 : 0.8343625125273946\n",
      "Loss at epoch 332 : 0.8160770587850946\n",
      "Loss at epoch 333 : 0.7982612269137761\n",
      "Loss at epoch 334 : 0.7809051191399038\n",
      "Loss at epoch 335 : 0.7639987610306993\n",
      "Loss at epoch 336 : 0.7475321328385303\n",
      "Loss at epoch 337 : 0.7314951980445681\n",
      "Loss at epoch 338 : 0.7158779292776423\n",
      "Loss at epoch 339 : 0.7006703317771815\n",
      "Loss at epoch 340 : 0.6858624645617126\n",
      "Loss at epoch 341 : 0.6714444594567766\n",
      "Loss at epoch 342 : 0.6574065381284919\n",
      "Loss at epoch 343 : 0.643739027261517\n",
      "Loss at epoch 344 : 0.6304323720128169\n",
      "Loss at epoch 345 : 0.6174771478655259\n",
      "Loss at epoch 346 : 0.6048640710003933\n",
      "Loss at epoch 347 : 0.5925840072957023\n",
      "Loss at epoch 348 : 0.5806279800602896\n",
      "Loss at epoch 349 : 0.5689871765982788\n",
      "Loss at epoch 350 : 0.557652953698393\n",
      "Loss at epoch 351 : 0.5466168421352657\n",
      "Loss at epoch 352 : 0.5358705502648965\n",
      "Loss at epoch 353 : 0.5254059667914487\n",
      "Loss at epoch 354 : 0.5152151627778059\n",
      "Loss at epoch 355 : 0.5052903929677464\n",
      "Loss at epoch 356 : 0.4956240964832921\n",
      "Loss at epoch 357 : 0.4862088969566111\n",
      "Loss at epoch 358 : 0.4770376021519436\n",
      "Loss at epoch 359 : 0.4681032031292438\n",
      "Loss at epoch 360 : 0.45939887299765075\n",
      "Loss at epoch 361 : 0.45091796530352296\n",
      "Loss at epoch 362 : 0.4426540120945114\n",
      "Loss at epoch 363 : 0.43460072169809183\n",
      "Loss at epoch 364 : 0.42675197625008526\n",
      "Loss at epoch 365 : 0.4191018290059211\n",
      "Loss at epoch 366 : 0.4116445014648302\n",
      "Loss at epoch 367 : 0.40437438033468015\n",
      "Loss at epoch 368 : 0.39728601436288635\n",
      "Loss at epoch 369 : 0.3903741110566501\n",
      "Loss at epoch 370 : 0.3836335333137457\n",
      "Loss at epoch 371 : 0.37705929598318066\n",
      "Loss at epoch 372 : 0.3706465623732744\n",
      "Loss at epoch 373 : 0.3643906407230334\n",
      "Loss at epoch 374 : 0.3582869806511567\n",
      "Loss at epoch 375 : 0.3523311695955845\n",
      "Loss at epoch 376 : 0.3465189292551387\n",
      "Loss at epoch 377 : 0.34084611204359816\n",
      "Loss at epoch 378 : 0.3353086975653852\n",
      "Loss at epoch 379 : 0.32990278912100507\n",
      "Loss at epoch 380 : 0.3246246102493988\n",
      "Loss at epoch 381 : 0.3194705013134776\n",
      "Loss at epoch 382 : 0.3144369161342879\n",
      "Loss at epoch 383 : 0.3095204186785205\n",
      "Loss at epoch 384 : 0.3047176798033642\n",
      "Loss at epoch 385 : 0.3000254740621108\n",
      "Loss at epoch 386 : 0.2954406765733273\n",
      "Loss at epoch 387 : 0.29096025995591496\n",
      "Loss at epoch 388 : 0.2865812913318841\n",
      "Loss at epoch 389 : 0.28230092939827967\n",
      "Loss at epoch 390 : 0.27811642156929306\n",
      "Loss at epoch 391 : 0.2740251011892607\n",
      "Loss at epoch 392 : 0.27002438481694885\n",
      "Loss at epoch 393 : 0.266111769581248\n",
      "Loss at epoch 394 : 0.26228483060815483\n",
      "Loss at epoch 395 : 0.25854121851871453\n",
      "Loss at epoch 396 : 0.2548786569973997\n",
      "Loss at epoch 397 : 0.25129494043023604\n",
      "Loss at epoch 398 : 0.24778793161185148\n",
      "Loss at epoch 399 : 0.24435555952047855\n",
      "Loss at epoch 400 : 0.24099581715986101\n",
      "Loss at epoch 401 : 0.23770675946689937\n",
      "Loss at epoch 402 : 0.2344865012838029\n",
      "Loss at epoch 403 : 0.23133321539345308\n",
      "Loss at epoch 404 : 0.22824513061663457\n",
      "Loss at epoch 405 : 0.22522052996972852\n",
      "Loss at epoch 406 : 0.22225774888145955\n",
      "Loss at epoch 407 : 0.21935517346724456\n",
      "Loss at epoch 408 : 0.21651123885967732\n",
      "Loss at epoch 409 : 0.21372442759368315\n",
      "Loss at epoch 410 : 0.2109932680448657\n",
      "Loss at epoch 411 : 0.2083163329195721\n",
      "Loss at epoch 412 : 0.20569223779521723\n",
      "Loss at epoch 413 : 0.20311963970940378\n",
      "Loss at epoch 414 : 0.20059723579640668\n",
      "Loss at epoch 415 : 0.19812376196959208\n",
      "Loss at epoch 416 : 0.19569799164837814\n",
      "Loss at epoch 417 : 0.19331873452834938\n",
      "Loss at epoch 418 : 0.1909848353931793\n",
      "Loss at epoch 419 : 0.1886951729670259\n",
      "Loss at epoch 420 : 0.1864486588061058\n",
      "Loss at epoch 421 : 0.18424423622817254\n",
      "Loss at epoch 422 : 0.18208087927865788\n",
      "Loss at epoch 423 : 0.17995759173226294\n",
      "Loss at epoch 424 : 0.1778734061288234\n",
      "Loss at epoch 425 : 0.175827382842292\n",
      "Loss at epoch 426 : 0.17381860918172395\n",
      "Loss at epoch 427 : 0.17184619852317692\n",
      "Loss at epoch 428 : 0.16990928947146788\n",
      "Loss at epoch 429 : 0.1680070450507614\n",
      "Loss at epoch 430 : 0.16613865192299523\n",
      "Loss at epoch 431 : 0.1643033196331775\n",
      "Loss at epoch 432 : 0.16250027988062166\n",
      "Loss at epoch 433 : 0.16072878581520733\n",
      "Loss at epoch 434 : 0.15898811135780536\n",
      "Loss at epoch 435 : 0.1572775505439989\n",
      "Loss at epoch 436 : 0.15559641689029519\n",
      "Loss at epoch 437 : 0.15394404278202672\n",
      "Loss at epoch 438 : 0.15231977888217918\n",
      "Loss at epoch 439 : 0.15072299356039998\n",
      "Loss at epoch 440 : 0.1491530723414759\n",
      "Loss at epoch 441 : 0.14760941737258648\n",
      "Loss at epoch 442 : 0.1460914469086634\n",
      "Loss at epoch 443 : 0.1445985948152183\n",
      "Loss at epoch 444 : 0.14313031008800692\n",
      "Loss at epoch 445 : 0.14168605638893855\n",
      "Loss at epoch 446 : 0.1402653115976483\n",
      "Loss at epoch 447 : 0.13886756737817277\n",
      "Loss at epoch 448 : 0.13749232876019182\n",
      "Loss at epoch 449 : 0.1361391137343202\n",
      "Loss at epoch 450 : 0.13480745286094253\n",
      "Loss at epoch 451 : 0.13349688889211475\n",
      "Loss at epoch 452 : 0.13220697640606352\n",
      "Loss at epoch 453 : 0.13093728145383765\n",
      "Loss at epoch 454 : 0.12968738121767898\n",
      "Loss at epoch 455 : 0.12845686368069484\n",
      "Loss at epoch 456 : 0.127245327307434\n",
      "Loss at epoch 457 : 0.12605238073497696\n",
      "Loss at epoch 458 : 0.12487764247417121\n",
      "Loss at epoch 459 : 0.12372074062065061\n",
      "Loss at epoch 460 : 0.12258131257529337\n",
      "Loss at epoch 461 : 0.12145900477378825\n",
      "Loss at epoch 462 : 0.12035347242498542\n",
      "Loss at epoch 463 : 0.11926437925772469\n",
      "Loss at epoch 464 : 0.11819139727584499\n",
      "Loss at epoch 465 : 0.11713420652108442\n",
      "Loss at epoch 466 : 0.11609249484360003\n",
      "Loss at epoch 467 : 0.11506595767983793\n",
      "Loss at epoch 468 : 0.11405429783749763\n",
      "Loss at epoch 469 : 0.11305722528734644\n",
      "Loss at epoch 470 : 0.11207445696164445\n",
      "Loss at epoch 471 : 0.11110571655895088\n",
      "Loss at epoch 472 : 0.11015073435509182\n",
      "Loss at epoch 473 : 0.10920924702007723\n",
      "Loss at epoch 474 : 0.10828099744076346\n",
      "Loss at epoch 475 : 0.10736573454905887\n",
      "Loss at epoch 476 : 0.10646321315549129\n",
      "Loss at epoch 477 : 0.10557319378794651\n",
      "Loss at epoch 478 : 0.104695442535405\n",
      "Loss at epoch 479 : 0.10382973089650627\n",
      "Loss at epoch 480 : 0.1029758356327771\n",
      "Loss at epoch 481 : 0.10213353862636528\n",
      "Loss at epoch 482 : 0.101302626742128\n",
      "Loss at epoch 483 : 0.10048289169392613\n",
      "Loss at epoch 484 : 0.09967412991498531\n",
      "Loss at epoch 485 : 0.09887614243218693\n",
      "Loss at epoch 486 : 0.09808873474415486\n",
      "Loss at epoch 487 : 0.09731171670301744\n",
      "Loss at epoch 488 : 0.09654490239971691\n",
      "Loss at epoch 489 : 0.0957881100527506\n",
      "Loss at epoch 490 : 0.09504116190023101\n",
      "Loss at epoch 491 : 0.09430388409515544\n",
      "Loss at epoch 492 : 0.09357610660377688\n",
      "Loss at epoch 493 : 0.09285766310698053\n",
      "Loss at epoch 494 : 0.09214839090455865\n",
      "Loss at epoch 495 : 0.09144813082229844\n",
      "Loss at epoch 496 : 0.09075672712178538\n",
      "Loss at epoch 497 : 0.09007402741283457\n",
      "Loss at epoch 498 : 0.08939988256847038\n",
      "Loss at epoch 499 : 0.08873414664236318\n",
      "Loss at epoch 500 : 0.08807667678865277\n",
      "Loss at epoch 501 : 0.08742733318407656\n",
      "Loss at epoch 502 : 0.0867859789523317\n",
      "Loss at epoch 503 : 0.08615248009059631\n",
      "Loss at epoch 504 : 0.08552670539814805\n",
      "Loss at epoch 505 : 0.08490852640700439\n",
      "Loss at epoch 506 : 0.08429781731452798\n",
      "Loss at epoch 507 : 0.08369445491793338\n",
      "Loss at epoch 508 : 0.0830983185506321\n",
      "Loss at epoch 509 : 0.08250929002036322\n",
      "Loss at epoch 510 : 0.08192725354904984\n",
      "Loss at epoch 511 : 0.08135209571433116\n",
      "Loss at epoch 512 : 0.08078370539271404\n",
      "Loss at epoch 513 : 0.08022197370429829\n",
      "Loss at epoch 514 : 0.07966679395902342\n",
      "Loss at epoch 515 : 0.07911806160439178\n",
      "Loss at epoch 516 : 0.07857567417462427\n",
      "Loss at epoch 517 : 0.07803953124120022\n",
      "Loss at epoch 518 : 0.07750953436474511\n",
      "Loss at epoch 519 : 0.07698558704821919\n",
      "Loss at epoch 520 : 0.07646759469137335\n",
      "Loss at epoch 521 : 0.07595546454642928\n",
      "Loss at epoch 522 : 0.07544910567494906\n",
      "Loss at epoch 523 : 0.07494842890585901\n",
      "Loss at epoch 524 : 0.0744533467945915\n",
      "Loss at epoch 525 : 0.07396377358331291\n",
      "Loss at epoch 526 : 0.07347962516220478\n",
      "Loss at epoch 527 : 0.07300081903176643\n",
      "Loss at epoch 528 : 0.07252727426610962\n",
      "Loss at epoch 529 : 0.07205891147721523\n",
      "Loss at epoch 530 : 0.07159565278012497\n",
      "Loss at epoch 531 : 0.0711374217590387\n",
      "Loss at epoch 532 : 0.07068414343429273\n",
      "Loss at epoch 533 : 0.0702357442301921\n",
      "Loss at epoch 534 : 0.069792151943673\n",
      "Loss at epoch 535 : 0.06935329571377098\n",
      "Loss at epoch 536 : 0.06891910599187126\n",
      "Loss at epoch 537 : 0.06848951451271842\n",
      "Loss at epoch 538 : 0.06806445426616574\n",
      "Loss at epoch 539 : 0.06764385946963963\n",
      "Loss at epoch 540 : 0.06722766554130184\n",
      "Loss at epoch 541 : 0.0668158090738877\n",
      "Loss at epoch 542 : 0.06640822780920218\n",
      "Loss at epoch 543 : 0.06600486061325424\n",
      "Loss at epoch 544 : 0.06560564745201278\n",
      "Loss at epoch 545 : 0.06521052936776586\n",
      "Loss at epoch 546 : 0.0648194484560661\n",
      "Loss at epoch 547 : 0.06443234784324613\n",
      "Loss at epoch 548 : 0.06404917166448841\n",
      "Loss at epoch 549 : 0.0636698650424352\n",
      "Loss at epoch 550 : 0.06329437406631926\n",
      "Loss at epoch 551 : 0.06292264577160789\n",
      "Loss at epoch 552 : 0.06255462812013945\n",
      "Loss at epoch 553 : 0.06219026998074323\n",
      "Loss at epoch 554 : 0.06182952111032742\n",
      "Loss at epoch 555 : 0.06147233213542323\n",
      "Loss at epoch 556 : 0.06111865453417278\n",
      "Loss at epoch 557 : 0.06076844061874696\n",
      "Loss at epoch 558 : 0.06042164351818578\n",
      "Loss at epoch 559 : 0.06007821716164451\n",
      "Loss at epoch 560 : 0.059738116262038676\n",
      "Loss at epoch 561 : 0.05940129630007613\n",
      "Loss at epoch 562 : 0.059067713508665215\n",
      "Loss at epoch 563 : 0.058737324857688844\n",
      "Loss at epoch 564 : 0.05841008803913675\n",
      "Loss at epoch 565 : 0.05808596145258392\n",
      "Loss at epoch 566 : 0.057764904191007646\n",
      "Loss at epoch 567 : 0.05744687602693396\n",
      "Loss at epoch 568 : 0.057131837398904026\n",
      "Loss at epoch 569 : 0.056819749398253926\n",
      "Loss at epoch 570 : 0.05651057375619721\n",
      "Loss at epoch 571 : 0.056204272831204743\n",
      "Loss at epoch 572 : 0.05590080959667228\n",
      "Loss at epoch 573 : 0.05560014762886978\n",
      "Loss at epoch 574 : 0.05530225109516389\n",
      "Loss at epoch 575 : 0.05500708474250711\n",
      "Loss at epoch 576 : 0.0547146138861878\n",
      "Loss at epoch 577 : 0.05442480439883293\n",
      "Loss at epoch 578 : 0.05413762269965658\n",
      "Loss at epoch 579 : 0.05385303574395143\n",
      "Loss at epoch 580 : 0.053571011012812364\n",
      "Loss at epoch 581 : 0.053291516503089574\n",
      "Loss at epoch 582 : 0.05301452071756513\n",
      "Loss at epoch 583 : 0.052739992655345146\n",
      "Loss at epoch 584 : 0.05246790180246519\n",
      "Loss at epoch 585 : 0.05219821812270126\n",
      "Loss at epoch 586 : 0.05193091204858263\n",
      "Loss at epoch 587 : 0.05166595447260047\n",
      "Loss at epoch 588 : 0.05140331673860836\n",
      "Loss at epoch 589 : 0.051142970633409376\n",
      "Loss at epoch 590 : 0.05088488837852571\n",
      "Loss at epoch 591 : 0.050629042622144585\n",
      "Loss at epoch 592 : 0.05037540643123979\n",
      "Loss at epoch 593 : 0.05012395328386017\n",
      "Loss at epoch 594 : 0.04987465706158431\n",
      "Loss at epoch 595 : 0.04962749204213536\n",
      "Loss at epoch 596 : 0.049382432892154084\n",
      "Loss at epoch 597 : 0.04913945466012419\n",
      "Loss at epoch 598 : 0.048898532769447445\n",
      "Loss at epoch 599 : 0.04865964301166507\n",
      "Loss at epoch 600 : 0.04842276153982163\n",
      "Loss at epoch 601 : 0.048187864861967966\n",
      "Loss at epoch 602 : 0.047954929834799716\n",
      "Loss at epoch 603 : 0.047723933657428455\n",
      "Loss at epoch 604 : 0.047494853865283064\n",
      "Loss at epoch 605 : 0.04726766832413626\n",
      "Loss at epoch 606 : 0.047042355224255455\n",
      "Loss at epoch 607 : 0.046818893074673545\n",
      "Loss at epoch 608 : 0.0465972606975783\n",
      "Loss at epoch 609 : 0.046377437222815246\n",
      "Loss at epoch 610 : 0.046159402082504374\n",
      "Loss at epoch 611 : 0.0459431350057653\n",
      "Loss at epoch 612 : 0.04572861601355095\n",
      "Loss at epoch 613 : 0.045515825413583885\n",
      "Loss at epoch 614 : 0.04530474379539674\n",
      "Loss at epoch 615 : 0.045095352025471866\n",
      "Loss at epoch 616 : 0.044887631242478646\n",
      "Loss at epoch 617 : 0.04468156285260576\n",
      "Loss at epoch 618 : 0.044477128524987225\n",
      "Loss at epoch 619 : 0.04427431018721969\n",
      "Loss at epoch 620 : 0.04407309002096859\n",
      "Loss at epoch 621 : 0.04387345045766006\n",
      "Loss at epoch 622 : 0.043675374174260456\n",
      "Loss at epoch 623 : 0.04347884408913679\n",
      "Loss at epoch 624 : 0.04328384335799988\n",
      "Loss at epoch 625 : 0.04309035536992574\n",
      "Loss at epoch 626 : 0.04289836374345582\n",
      "Loss at epoch 627 : 0.042707852322772764\n",
      "Loss at epoch 628 : 0.04251880517395039\n",
      "Loss at epoch 629 : 0.04233120658127599\n",
      "Loss at epoch 630 : 0.04214504104364479\n",
      "Loss at epoch 631 : 0.04196029327102298\n",
      "Loss at epoch 632 : 0.041776948180978164\n",
      "Loss at epoch 633 : 0.04159499089527821\n",
      "Loss at epoch 634 : 0.04141440673655332\n",
      "Loss at epoch 635 : 0.04123518122502255\n",
      "Loss at epoch 636 : 0.041057300075282195\n",
      "Loss at epoch 637 : 0.040880749193155444\n",
      "Loss at epoch 638 : 0.040705514672601584\n",
      "Loss at epoch 639 : 0.04053158279268271\n",
      "Loss at epoch 640 : 0.040358940014588784\n",
      "Loss at epoch 641 : 0.04018757297871736\n",
      "Loss at epoch 642 : 0.04001746850180871\n",
      "Loss at epoch 643 : 0.039848613574134\n",
      "Loss at epoch 644 : 0.0396809953567353\n",
      "Loss at epoch 645 : 0.039514601178718745\n",
      "Loss at epoch 646 : 0.03934941853459474\n",
      "Loss at epoch 647 : 0.03918543508167073\n",
      "Loss at epoch 648 : 0.03902263863748878\n",
      "Loss at epoch 649 : 0.038861017177312794\n",
      "Loss at epoch 650 : 0.03870055883165968\n",
      "Loss at epoch 651 : 0.03854125188387757\n",
      "Loss at epoch 652 : 0.03838308476776624\n",
      "Loss at epoch 653 : 0.03822604606524273\n",
      "Loss at epoch 654 : 0.03807012450404725\n",
      "Loss at epoch 655 : 0.03791530895549236\n",
      "Loss at epoch 656 : 0.03776158843225192\n",
      "Loss at epoch 657 : 0.03760895208618929\n",
      "Loss at epoch 658 : 0.03745738920622599\n",
      "Loss at epoch 659 : 0.037306889216246905\n",
      "Loss at epoch 660 : 0.037157441673043856\n",
      "Loss at epoch 661 : 0.03700903626429545\n",
      "Loss at epoch 662 : 0.036861662806583054\n",
      "Loss at epoch 663 : 0.036715311243441545\n",
      "Loss at epoch 664 : 0.03656997164344473\n",
      "Loss at epoch 665 : 0.036425634198324564\n",
      "Loss at epoch 666 : 0.036282289221123296\n",
      "Loss at epoch 667 : 0.03613992714437808\n",
      "Loss at epoch 668 : 0.03599853851833733\n",
      "Loss at epoch 669 : 0.03585811400920798\n",
      "Loss at epoch 670 : 0.035718644397434385\n",
      "Loss at epoch 671 : 0.035580120576004744\n",
      "Loss at epoch 672 : 0.03544253354878961\n",
      "Loss at epoch 673 : 0.03530587442890693\n",
      "Loss at epoch 674 : 0.03517013443711672\n",
      "Loss at epoch 675 : 0.03503530490024233\n",
      "Loss at epoch 676 : 0.03490137724961943\n",
      "Loss at epoch 677 : 0.03476834301957097\n",
      "Loss at epoch 678 : 0.03463619384590922\n",
      "Loss at epoch 679 : 0.034504921464461394\n",
      "Loss at epoch 680 : 0.034374517709622826\n",
      "Loss at epoch 681 : 0.034244974512932055\n",
      "Loss at epoch 682 : 0.03411628390167217\n",
      "Loss at epoch 683 : 0.03398843799749419\n",
      "Loss at epoch 684 : 0.03386142901506457\n",
      "Loss at epoch 685 : 0.033735249260735053\n",
      "Loss at epoch 686 : 0.03360989113123429\n",
      "Loss at epoch 687 : 0.03348534711238218\n",
      "Loss at epoch 688 : 0.03336160977782506\n",
      "Loss at epoch 689 : 0.03323867178779215\n",
      "Loss at epoch 690 : 0.03311652588787231\n",
      "Loss at epoch 691 : 0.03299516490781122\n",
      "Loss at epoch 692 : 0.03287458176032873\n",
      "Loss at epoch 693 : 0.03275476943995491\n",
      "Loss at epoch 694 : 0.032635721021885604\n",
      "Loss at epoch 695 : 0.03251742966085742\n",
      "Loss at epoch 696 : 0.03239988859003941\n",
      "Loss at epoch 697 : 0.032283091119944506\n",
      "Loss at epoch 698 : 0.03216703063735806\n",
      "Loss at epoch 699 : 0.03205170060428331\n",
      "Loss at epoch 700 : 0.031937094556904064\n",
      "Loss at epoch 701 : 0.03182320610456518\n",
      "Loss at epoch 702 : 0.031710028928767835\n",
      "Loss at epoch 703 : 0.03159755678218226\n",
      "Loss at epoch 704 : 0.03148578348767555\n",
      "Loss at epoch 705 : 0.031374702937355066\n",
      "Loss at epoch 706 : 0.03126430909162786\n",
      "Loss at epoch 707 : 0.03115459597827391\n",
      "Loss at epoch 708 : 0.031045557691534973\n",
      "Loss at epoch 709 : 0.030937188391217722\n",
      "Loss at epoch 710 : 0.03082948230181068\n",
      "Loss at epoch 711 : 0.030722433711615052\n",
      "Loss at epoch 712 : 0.030616036971890373\n",
      "Loss at epoch 713 : 0.03051028649601171\n",
      "Loss at epoch 714 : 0.03040517675864179\n",
      "Loss at epoch 715 : 0.030300702294914997\n",
      "Loss at epoch 716 : 0.030196857699634504\n",
      "Loss at epoch 717 : 0.030093637626481838\n",
      "Loss at epoch 718 : 0.02999103678723854\n",
      "Loss at epoch 719 : 0.029889049951020603\n",
      "Loss at epoch 720 : 0.029787671943523455\n",
      "Loss at epoch 721 : 0.0296868976462799\n",
      "Loss at epoch 722 : 0.029586721995928343\n",
      "Loss at epoch 723 : 0.02948713998349341\n",
      "Loss at epoch 724 : 0.029388146653676436\n",
      "Loss at epoch 725 : 0.029289737104157602\n",
      "Loss at epoch 726 : 0.029191906484908468\n",
      "Loss at epoch 727 : 0.029094649997514727\n",
      "Loss at epoch 728 : 0.028997962894509628\n",
      "Loss at epoch 729 : 0.02890184047871735\n",
      "Loss at epoch 730 : 0.02880627810260593\n",
      "Loss at epoch 731 : 0.028711271167650577\n",
      "Loss at epoch 732 : 0.028616815123706445\n",
      "Loss at epoch 733 : 0.02852290546839009\n",
      "Loss at epoch 734 : 0.028429537746471147\n",
      "Loss at epoch 735 : 0.028336707549272692\n",
      "Loss at epoch 736 : 0.028244410514080262\n",
      "Loss at epoch 737 : 0.028152642323560396\n",
      "Loss at epoch 738 : 0.028061398705186938\n",
      "Loss at epoch 739 : 0.027970675430676533\n",
      "Loss at epoch 740 : 0.027880468315432304\n",
      "Loss at epoch 741 : 0.02779077321799521\n",
      "Loss at epoch 742 : 0.02770158603950428\n",
      "Loss at epoch 743 : 0.02761290272316436\n",
      "Loss at epoch 744 : 0.027524719253721734\n",
      "Loss at epoch 745 : 0.027437031656947036\n",
      "Loss at epoch 746 : 0.02734983599912661\n",
      "Loss at epoch 747 : 0.027263128386560174\n",
      "Loss at epoch 748 : 0.02717690496506688\n",
      "Loss at epoch 749 : 0.02709116191949756\n",
      "Loss at epoch 750 : 0.02700589547325445\n",
      "Loss at epoch 751 : 0.02692110188781842\n",
      "Loss at epoch 752 : 0.026836777462281483\n",
      "Loss at epoch 753 : 0.026752918532888078\n",
      "Loss at epoch 754 : 0.02666952147258122\n",
      "Loss at epoch 755 : 0.026586582690555617\n",
      "Loss at epoch 756 : 0.026504098631817817\n",
      "Loss at epoch 757 : 0.026422065776751873\n",
      "Loss at epoch 758 : 0.02634048064069149\n",
      "Loss at epoch 759 : 0.026259339773498258\n",
      "Loss at epoch 760 : 0.0261786397591457\n",
      "Loss at epoch 761 : 0.026098377215309526\n",
      "Loss at epoch 762 : 0.02601854879296302\n",
      "Loss at epoch 763 : 0.02593915117597932\n",
      "Loss at epoch 764 : 0.02586018108073756\n",
      "Loss at epoch 765 : 0.025781635255736138\n",
      "Loss at epoch 766 : 0.025703510481210664\n",
      "Loss at epoch 767 : 0.025625803568757433\n",
      "Loss at epoch 768 : 0.02554851136096139\n",
      "Loss at epoch 769 : 0.025471630731030762\n",
      "Loss at epoch 770 : 0.025395158582435382\n",
      "Loss at epoch 771 : 0.025319091848550808\n",
      "Loss at epoch 772 : 0.025243427492307055\n",
      "Loss at epoch 773 : 0.02516816250584189\n",
      "Loss at epoch 774 : 0.0250932939101599\n",
      "Loss at epoch 775 : 0.025018818754795016\n",
      "Loss at epoch 776 : 0.02494473411747833\n",
      "Loss at epoch 777 : 0.024871037103810494\n",
      "Loss at epoch 778 : 0.024797724846938388\n",
      "Loss at epoch 779 : 0.024724794507236234\n",
      "Loss at epoch 780 : 0.02465224327199099\n",
      "Loss at epoch 781 : 0.024580068355092202\n",
      "Loss at epoch 782 : 0.02450826699672571\n",
      "Loss at epoch 783 : 0.024436836463072105\n",
      "Loss at epoch 784 : 0.024365774046008516\n",
      "Loss at epoch 785 : 0.02429507706281485\n",
      "Loss at epoch 786 : 0.024224742855884107\n",
      "Loss at epoch 787 : 0.024154768792436093\n",
      "Loss at epoch 788 : 0.024085152264235212\n",
      "Loss at epoch 789 : 0.02401589068731225\n",
      "Loss at epoch 790 : 0.023946981501689682\n",
      "Loss at epoch 791 : 0.023878422171110393\n",
      "Loss at epoch 792 : 0.02381021018277024\n",
      "Loss at epoch 793 : 0.0237423430470544\n",
      "Loss at epoch 794 : 0.023674818297276707\n",
      "Loss at epoch 795 : 0.023607633489422804\n",
      "Loss at epoch 796 : 0.02354078620189623\n",
      "Loss at epoch 797 : 0.023474274035268673\n",
      "Loss at epoch 798 : 0.02340809461203256\n",
      "Loss at epoch 799 : 0.023342245576357366\n",
      "Loss at epoch 800 : 0.02327672459384937\n",
      "Loss at epoch 801 : 0.023211529351313546\n",
      "Loss at epoch 802 : 0.02314665755652016\n",
      "Loss at epoch 803 : 0.023082106937972528\n",
      "Loss at epoch 804 : 0.02301787524467938\n",
      "Loss at epoch 805 : 0.022953960245929265\n",
      "Loss at epoch 806 : 0.022890359731068205\n",
      "Loss at epoch 807 : 0.022827071509279772\n",
      "Loss at epoch 808 : 0.022764093409369142\n",
      "Loss at epoch 809 : 0.02270142327954817\n",
      "Loss at epoch 810 : 0.02263905898722507\n",
      "Loss at epoch 811 : 0.022576998418795187\n",
      "Loss at epoch 812 : 0.022515239479435595\n",
      "Loss at epoch 813 : 0.0224537800929017\n",
      "Loss at epoch 814 : 0.022392618201326694\n",
      "Loss at epoch 815 : 0.02233175176502334\n",
      "Loss at epoch 816 : 0.022271178762288422\n",
      "Loss at epoch 817 : 0.022210897189209752\n",
      "Loss at epoch 818 : 0.022150905059475583\n",
      "Loss at epoch 819 : 0.022091200404185808\n",
      "Loss at epoch 820 : 0.022031781271666745\n",
      "Loss at epoch 821 : 0.02197264572728707\n",
      "Loss at epoch 822 : 0.021913791853276923\n",
      "Loss at epoch 823 : 0.021855217748548574\n",
      "Loss at epoch 824 : 0.02179692152851994\n",
      "Loss at epoch 825 : 0.021738901324939826\n",
      "Loss at epoch 826 : 0.02168115528571567\n",
      "Loss at epoch 827 : 0.0216236815747433\n",
      "Loss at epoch 828 : 0.021566478371738584\n",
      "Loss at epoch 829 : 0.021509543872071633\n",
      "Loss at epoch 830 : 0.02145287628660257\n",
      "Loss at epoch 831 : 0.02139647384151986\n",
      "Loss at epoch 832 : 0.021340334778179905\n",
      "Loss at epoch 833 : 0.02128445735294933\n",
      "Loss at epoch 834 : 0.0212288398370487\n",
      "Loss at epoch 835 : 0.021173480516398373\n",
      "Loss at epoch 836 : 0.021118377691466392\n",
      "Loss at epoch 837 : 0.021063529677117693\n",
      "Loss at epoch 838 : 0.02100893480246584\n",
      "Loss at epoch 839 : 0.020954591410725927\n",
      "Loss at epoch 840 : 0.020900497859069708\n",
      "Loss at epoch 841 : 0.020846652518482153\n",
      "Loss at epoch 842 : 0.020793053773620312\n",
      "Loss at epoch 843 : 0.020739700022672933\n",
      "Loss at epoch 844 : 0.02068658967722261\n",
      "Loss at epoch 845 : 0.020633721162109313\n",
      "Loss at epoch 846 : 0.020581092915295217\n",
      "Loss at epoch 847 : 0.02052870338773212\n",
      "Loss at epoch 848 : 0.02047655104322891\n",
      "Loss at epoch 849 : 0.02042463435832214\n",
      "Loss at epoch 850 : 0.020372951822147288\n",
      "Loss at epoch 851 : 0.020321501936311718\n",
      "Loss at epoch 852 : 0.02027028321476891\n",
      "Loss at epoch 853 : 0.020219294183695315\n",
      "Loss at epoch 854 : 0.0201685333813666\n",
      "Loss at epoch 855 : 0.020117999358037653\n",
      "Loss at epoch 856 : 0.02006769067582214\n",
      "Loss at epoch 857 : 0.020017605908574466\n",
      "Loss at epoch 858 : 0.019967743641773145\n",
      "Loss at epoch 859 : 0.01991810247240498\n",
      "Loss at epoch 860 : 0.01986868100885101\n",
      "Loss at epoch 861 : 0.019819477870773594\n",
      "Loss at epoch 862 : 0.019770491689005083\n",
      "Loss at epoch 863 : 0.019721721105437427\n",
      "Loss at epoch 864 : 0.019673164772913414\n",
      "Loss at epoch 865 : 0.019624821355118748\n",
      "Loss at epoch 866 : 0.019576689526475732\n",
      "Loss at epoch 867 : 0.019528767972038393\n",
      "Loss at epoch 868 : 0.019481055387387963\n",
      "Loss at epoch 869 : 0.01943355047853023\n",
      "Loss at epoch 870 : 0.019386251961794367\n",
      "Loss at epoch 871 : 0.019339158563731935\n",
      "Loss at epoch 872 : 0.01929226902101787\n",
      "Loss at epoch 873 : 0.019245582080352274\n",
      "Loss at epoch 874 : 0.019199096498363454\n",
      "Loss at epoch 875 : 0.019152811041511973\n",
      "Loss at epoch 876 : 0.019106724485995534\n",
      "Loss at epoch 877 : 0.019060835617655727\n",
      "Loss at epoch 878 : 0.01901514323188499\n",
      "Loss at epoch 879 : 0.018969646133534905\n",
      "Loss at epoch 880 : 0.01892434313682584\n",
      "Loss at epoch 881 : 0.01887923306525724\n",
      "Loss at epoch 882 : 0.018834314751518998\n",
      "Loss at epoch 883 : 0.018789587037403986\n",
      "Loss at epoch 884 : 0.018745048773721285\n",
      "Loss at epoch 885 : 0.0187006988202107\n",
      "Loss at epoch 886 : 0.018656536045458062\n",
      "Loss at epoch 887 : 0.018612559326811422\n",
      "Loss at epoch 888 : 0.018568767550298362\n",
      "Loss at epoch 889 : 0.018525159610544\n",
      "Loss at epoch 890 : 0.018481734410690072\n",
      "Loss at epoch 891 : 0.018438490862315103\n",
      "Loss at epoch 892 : 0.018395427885354657\n",
      "Loss at epoch 893 : 0.018352544408023704\n",
      "Loss at epoch 894 : 0.018309839366738603\n",
      "Loss at epoch 895 : 0.018267311706040977\n",
      "Loss at epoch 896 : 0.018224960378521657\n",
      "Loss at epoch 897 : 0.01818278434474583\n",
      "Loss at epoch 898 : 0.018140782573179047\n",
      "Loss at epoch 899 : 0.01809895404011397\n",
      "Loss at epoch 900 : 0.018057297729597535\n",
      "Loss at epoch 901 : 0.018015812633359933\n",
      "Loss at epoch 902 : 0.017974497750742877\n",
      "Loss at epoch 903 : 0.017933352088630148\n",
      "Loss at epoch 904 : 0.017892374661377896\n",
      "Loss at epoch 905 : 0.017851564490746034\n",
      "Loss at epoch 906 : 0.017810920605830415\n",
      "Loss at epoch 907 : 0.017770442042995677\n",
      "Loss at epoch 908 : 0.017730127845808793\n",
      "Loss at epoch 909 : 0.01768997706497339\n",
      "Loss at epoch 910 : 0.017649988758264668\n",
      "Loss at epoch 911 : 0.017610161990465212\n",
      "Loss at epoch 912 : 0.01757049583330126\n",
      "Loss at epoch 913 : 0.017530989365379827\n",
      "Loss at epoch 914 : 0.017491641672126464\n",
      "Loss at epoch 915 : 0.01745245184572363\n",
      "Loss at epoch 916 : 0.017413418985049667\n",
      "Loss at epoch 917 : 0.017374542195618636\n",
      "Loss at epoch 918 : 0.017335820589520642\n",
      "Loss at epoch 919 : 0.017297253285362663\n",
      "Loss at epoch 920 : 0.017258839408210346\n",
      "Loss at epoch 921 : 0.017220578089530026\n",
      "Loss at epoch 922 : 0.017182468467131676\n",
      "Loss at epoch 923 : 0.017144509685112504\n",
      "Loss at epoch 924 : 0.017106700893800536\n",
      "Loss at epoch 925 : 0.017069041249699504\n",
      "Loss at epoch 926 : 0.017031529915434065\n",
      "Loss at epoch 927 : 0.016994166059695545\n",
      "Loss at epoch 928 : 0.016956948857188046\n",
      "Loss at epoch 929 : 0.016919877488575638\n",
      "Loss at epoch 930 : 0.01688295114042971\n",
      "Loss at epoch 931 : 0.01684616900517681\n",
      "Loss at epoch 932 : 0.016809530281047258\n",
      "Loss at epoch 933 : 0.016773034172024387\n",
      "Loss at epoch 934 : 0.01673667988779376\n",
      "Loss at epoch 935 : 0.016700466643693658\n",
      "Loss at epoch 936 : 0.016664393660665442\n",
      "Loss at epoch 937 : 0.01662846016520477\n",
      "Loss at epoch 938 : 0.01659266538931334\n",
      "Loss at epoch 939 : 0.01655700857045072\n",
      "Loss at epoch 940 : 0.01652148895148731\n",
      "Loss at epoch 941 : 0.01648610578065734\n",
      "Loss at epoch 942 : 0.016450858311512186\n",
      "Loss at epoch 943 : 0.016415745802874915\n",
      "Loss at epoch 944 : 0.01638076751879435\n",
      "Loss at epoch 945 : 0.016345922728500464\n",
      "Loss at epoch 946 : 0.016311210706359363\n",
      "Loss at epoch 947 : 0.016276630731829642\n",
      "Loss at epoch 948 : 0.016242182089418425\n",
      "Loss at epoch 949 : 0.016207864068638254\n",
      "Loss at epoch 950 : 0.01617367596396444\n",
      "Loss at epoch 951 : 0.016139617074792407\n",
      "Loss at epoch 952 : 0.016105686705395958\n",
      "Loss at epoch 953 : 0.01607188416488586\n",
      "Loss at epoch 954 : 0.01603820876716864\n",
      "Loss at epoch 955 : 0.016004659830905807\n",
      "Loss at epoch 956 : 0.015971236679473828\n",
      "Loss at epoch 957 : 0.01593793864092415\n",
      "Loss at epoch 958 : 0.015904765047943643\n",
      "Loss at epoch 959 : 0.01587171523781567\n",
      "Loss at epoch 960 : 0.015838788552381286\n",
      "Loss at epoch 961 : 0.015805984338001067\n",
      "Loss at epoch 962 : 0.015773301945517033\n",
      "Loss at epoch 963 : 0.01574074073021524\n",
      "Loss at epoch 964 : 0.015708300051788502\n",
      "Loss at epoch 965 : 0.015675979274299638\n",
      "Loss at epoch 966 : 0.01564377776614496\n",
      "Loss at epoch 967 : 0.015611694900018364\n",
      "Loss at epoch 968 : 0.015579730052875329\n",
      "Loss at epoch 969 : 0.015547882605897694\n",
      "Loss at epoch 970 : 0.015516151944458618\n",
      "Loss at epoch 971 : 0.015484537458087932\n",
      "Loss at epoch 972 : 0.015453038540437567\n",
      "Loss at epoch 973 : 0.015421654589247795\n",
      "Loss at epoch 974 : 0.01539038500631345\n",
      "Loss at epoch 975 : 0.015359229197450409\n",
      "Loss at epoch 976 : 0.01532818657246271\n",
      "Loss at epoch 977 : 0.015297256545109923\n",
      "Loss at epoch 978 : 0.01526643853307455\n",
      "Loss at epoch 979 : 0.01523573195793008\n",
      "Loss at epoch 980 : 0.015205136245109017\n",
      "Loss at epoch 981 : 0.015174650823871682\n",
      "Loss at epoch 982 : 0.015144275127274794\n",
      "Loss at epoch 983 : 0.015114008592140803\n",
      "Loss at epoch 984 : 0.015083850659027148\n",
      "Loss at epoch 985 : 0.01505380077219607\n",
      "Loss at epoch 986 : 0.015023858379584601\n",
      "Loss at epoch 987 : 0.014994022932774754\n",
      "Loss at epoch 988 : 0.014964293886964426\n",
      "Loss at epoch 989 : 0.014934670700937736\n",
      "Loss at epoch 990 : 0.014905152837036662\n",
      "Loss at epoch 991 : 0.014875739761132212\n",
      "Loss at epoch 992 : 0.014846430942595993\n",
      "Loss at epoch 993 : 0.014817225854272444\n",
      "Loss at epoch 994 : 0.01478812397245083\n",
      "Loss at epoch 995 : 0.014759124776837839\n",
      "Loss at epoch 996 : 0.014730227750530248\n",
      "Loss at epoch 997 : 0.014701432379987826\n",
      "Loss at epoch 998 : 0.014672738155006823\n",
      "Loss at epoch 999 : 0.014644144568693295\n"
     ]
    }
   ],
   "source": [
    "model = MLP(3, [4, 4, 1])\n",
    "\n",
    "for i in range(1000):\n",
    "    y_preds = [model(x) for x in X]\n",
    "    loss = sum((yout - ygt)**2 for ygt, yout in zip(y, y_preds))\n",
    "\n",
    "    print(f\"Loss at epoch {i} : {loss.data}\")\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.grad = 0.0\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.data -= 0.01 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9671370142193432),\n",
       " Value(data=-0.9619210726520783),\n",
       " Value(data=0.9657869112685732),\n",
       " Value(data=-0.9714754083319519),\n",
       " Value(data=-0.9491769914135819),\n",
       " Value(data=-0.9939774861278045),\n",
       " Value(data=-0.9903757145177627),\n",
       " Value(data=-0.9616786991685817),\n",
       " Value(data=-0.9892099249545276),\n",
       " Value(data=0.9236249209959592)]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=1.0),\n",
       " Value(data=-0.9999999999976916),\n",
       " Value(data=1.0),\n",
       " Value(data=-1.0),\n",
       " Value(data=-1.0),\n",
       " Value(data=-1.0),\n",
       " Value(data=-1.0),\n",
       " Value(data=-1.0),\n",
       " Value(data=-1.0),\n",
       " Value(data=-1.0)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    p.data -= 0.01 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=8.179001024089946)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = sum((yout - ygt)**2 for ygt, yout in zip(y, y_preds))\n",
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrograd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
